\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{linear algebra}

${ add_statement(

'Linear Transformation',

r'''
A linear transformation on a @space@ G is a @bijection@ mapping from a @coordinate system@ $ \Lambda $ on G mapping to a @state vector space@ V to a coordinate system $\Psi$ on same G mapping to state vector space W of same dimension as V, such that each @vector@ in @standard basis@ of W is a @linear combination@ of all vectors in V.
''',

r'''
A simple tool for moving b/w representations of space.
Some popular examples of linear transformations are rotation, scale, sheer.

For instance in a 2D state vector space, if standard basis of W say $ \hat{o}, \hat{p} $ are linear combinations of standard basis of V say $ \hat{d}, \hat{f} $ as follows
\[  \hat{o} = 3 * \hat{d} + 2 * \hat{f} \]
\[  \hat{p} = 0 * \hat{d} + 2 * \hat{f} \]
Note that
\begin{enumerate}[nolistsep]
  \item In V $ \hat{o}, \hat{p} $ represent $ \begin{bmatrix} 1 \\ 0 \end{bmatrix} and \begin{bmatrix} 0 \\ 1 \end{bmatrix} $ repsectively.
  \item In W $ \hat{d}, \hat{f} $ represent $ \begin{bmatrix} 1 \\ 0 \end{bmatrix} and \begin{bmatrix} 0 \\ 1 \end{bmatrix} $ repsectively.
\end{enumerate}
But V and W are different representations of the same space. Therfore,
\begin{enumerate}[nolistsep]
  \item $ \hat{o}, \hat{p} $ represents a same vector as $ \hat{d}, \hat{f} $. So they are indentical in this context.
  \item $ \hat{o}, \hat{p} $ represents a different point than $ \hat{d}, \hat{f} $. So they are non identical in this context.
\end{enumerate}
Therefore when we say two vectors are equal we should be careful about the context of the discussion.

Suppose a point $ \eta $ is represented by $ \begin{bmatrix} 4 \\ 10 \end{bmatrix} $ in $ \Psi $.
That means P is represented by $ 4 * \hat{o} + 10 * \hat{p} $.
Now to get representation of the same point $ \eta $ in $ \Lambda $ (remember points do not change with anything) we just substitute the expressions of $ \hat{o}, \hat{p} $ in terms of $ \hat{d}, \hat{f} $ as given above, i.e. $ 4 * (3 * \hat{d} + 2 * \hat{f}) + 10 * (0 * \hat{d} + 2 * \hat{f}) $ which gives $ 12 * \hat{d} + 28 * \hat{f} $.
This can be written as
\[
  4 * \begin{bmatrix} 3 \\ 2 \end{bmatrix} + 10 * \begin{bmatrix} 0 \\ 2 \end{bmatrix}
\]
which can be further packaged as a matrix multiplication.
\[
  \begin{bmatrix} 3 & 0 \\ 2 & 2 \end{bmatrix} \begin{bmatrix} 4 \\ 10 \end{bmatrix}
\]
This is why the matrix multiplication was invented. Mind Blown!!
Therefore columns in a matrix can be seen as the representations of standard basis of new coordinate system in the old one.

Therefore the linear tranform can be seen as a way to do the following things
\begin{enumerate}[nolistsep]
  \item Given a transformed coordinate system i.e. given standard basis of new coordinate system in terms of the old one, we can get the representation for the same point in two different systems.
  \item Visualize moving a vector $ \lambda $ representing a point A in $ \Lambda $ to the point B represented in $ \Psi $ by $ \psi $ which is numerically equal to $ \lambda $.
\end{enumerate}
Donot mix these two.

Misconceptions busting.
\begin{enumerate}[nolistsep]
  \item Rotation is not a property of a body, it is a property of a pair of coordinate systems. One of the coordinate systems can be attached to a body.
  \item Rotation is no some complicated thing, it is just a linear transformation. The columns of the matrix representing rotation transform are just the places where the new standard basis land in old coordinate system.
  \item There is no such thing as multiplying a vector on its right. Dimensions don't match for multiplication to occur.
  \item Linear transformation is not about raw numbers, the idea comes from visualing space and trying to play with it.
\end{enumerate}


''',

r'''
\proofbydefinition
'''

) }



TODO
\begin{enumerate}[nolistsep]
  \item idea of linear transform
  \item writing linear transform and composition of linear transforms as matrix and product of matrices, building matrix as tool
  \item proof of composition of linear transform is a linear transform, algebraically and geometrically
  \item misconceptions: multiplying matrices on left of vectors vs \xcancel{right of vectors}
  \item misconceptions: animating vectors vs \xcancel{animating coordinate frames}
  \item latest frame vs initial frame
  \item rotation as a special case of linear transform, rotation in 3d always has a one and only one axis passing through origin proof (using linear transform + constant L2 norm + two vector defn of plane + normal to plane)
  \item rotation in 2d only around point on origin
  \item determinant, descriminant whatever
  \item dot product, cross product
  \item linearly dependent/independent
  \item inverses
  \item column spaces, rank
  \item null spaces
  \item eigen stuff, eigen basis
  \item orthogonal, orthonormal
  \item diagnoal
  \item +ve definite, semi +ve definite
  \item quaternions
  \item SVD, other easy useful decompositions
\end{enumerate}

%% \chapter{co-ordinate systems and vectors}
%% We can think of \textbf{each number in vector as the signed length in each of the axes of an N dimensional co-ordinate frame}.
%% This way a numerical form of vector can be used to represent a point in space of appropriate dimension and a point can be used to visualize a numerical form of vector.

%% \begin{figure}[ht]
%%   \centering
%%   \begin{tikzpicture}
%%     \draw (-3, 0) ellipse (2cm and 1cm);
%%     \draw (-3, 0) node {Numerical};
%%     \draw (3, 0) ellipse (2cm and 1cm);
%%     \draw (3, 0) node {Co-ordinate};
%%     \draw[->] (-3, 1) -- (3, 1);
%%     \draw (0, 1.5) node {visualize};
%%     \draw[->] (3, -1) -- (-3, -1);
%%     \draw (0, -1.5) node { represent };
%%   \end{tikzpicture}
%% \end{figure}


%% For example a vector $ \blockcomment{Column-Vector: 2, -3} \begin{pmatrix} 2 \\  -3 \end{pmatrix} $  has dimensionality of 2, hence called a 2D vector and can be used to represent a point in a 2D co-ordinate space formed by 2D co-ordinate frame which consists of 2 axes. The point can be arrived at starting from the origin and moving 2 units along first axis and -3 units along second axis.

%% \begin{figure}[ht]
%%   \centering
%%   \begin{tikzpicture}
%%     \draw[step=1cm,gray,very thin] (-2.9,-3.9) grid (2.9,1.9);
%%     \draw[->] (-3, 0) -- (3, 0) node[anchor=north] {axis 1};
%%     \draw[->] (0, -4) -- (0, 2) node[anchor=east] {axis 2};
%%     \draw[thick, ->] (0, 0) -- (2, -3) node[anchor=west] { $ \blockcomment{Column-Vector: 2, -3} \begin{pmatrix} 2 \\  -3 \end{pmatrix} $  };
%%   \end{tikzpicture}
%% \end{figure}

%% Please note that
%% \begin{enumerate}
%%   \item There is no need to name these axes X and Y.
%%   \item There is no need for the axes to be perpendicular.
%%   \item No need for 1 unit along first axis to be same as 1 unit along second axis.
%%   \item No need for units of first axis to be same as the second one
%% \end{enumerate}

%% \chapter{Why like this?}
%% Well there is no specific reason. Just that it seems natural and seems to be useful to solve many problems in the world.

%% Probably the co-ordinate system was first developed to represent the objects in the real world with numbers. A co-ordinate system with dimensionality up to 3 makes sense in the real world. The ones with dimensionality 4 or above are not natural, as in we don't see physical manifestations of such systems. Umm... may be space time can be thought of as a 4D co-ordinate system with the 4th axis being time.

%% Anyway the most natural way to think about co-ordinate systems is to imagine a 3D co-ordinate system with axes placed spatially mutually perpendicularly and origin at the interchapter of all of them.
%% \textbf{A (yet another) different way to think about vectors}
%% Well consider a 2D co-ordinate system. Say we draw the vector $ \blockcomment{Column-Vector: 2, -3} \begin{pmatrix} 2 \\  -3 \end{pmatrix} $ in that system.

%% \begin{figure}[h]
%%   \centering
%%   \begin{tikzpicture}
%%     \draw[step=1cm,gray,very thin] (-2.9,-3.9) grid (2.9,1.9);
%%     \draw[->] (-3, 0) -- (3, 0) node[anchor=north] {axis 1};
%%     \draw[->] (0, -4) -- (0, 2) node[anchor=east] {axis 2};
%%     \draw[thick, ->] (0, 0) -- (2, -3) node[anchor=west] { $ \blockcomment{Column-Vector: 2, -3} \begin{pmatrix} 2 \\  -3 \end{pmatrix} $  };
%%   \end{tikzpicture}
%% \end{figure}

%% We know that each number in the list (the vector) is the signed length that we need to travel to get to the point represented by the vector.
%% Well we can also `think' that \textbf{each number represents the `scalar' that scales some very fundamental vectors that belong to the system and by adding the scaled versions of those vectors we get the same point}. These fundamental vectors can be thought of as extension of the definition of co-ordinate system itself, in that each axis of a co-ordinate system has one and only one vector associated with it.

%% This idea of \textbf{scaling and adding things is known as linear combination} in general. In linear algebra the things are vectors. Most ideas in linear algebra build up on this idea of scaling and adding some fundamental vectors (i.e. linear combination of vectors). In the regular 2D co-ordinate system these vectors are $ \ihat $ and $ \jhat $.

%% So a linear combination of $ \ihat $ and $ \jhat $ can be represented symbolically as \[
%% \alpha * \ihat + \beta * \jhat
%% .\] where $ \alpha $ and $ \beta $ are any real numbers.

%% The word comes from I guess the fact that there are other types of combinations like polynomial combination, exponential combination, logarithmic combination and what not. The most simplest of them all for us humans seems to be linear combination. And we know from experience that simple is fun and often powerful.

%% \chapter{What does `linearly dependent' and `linearly independent' mean?}

%% A set of vectors are said to be \textbf{linearly dependent iff one of them can be expressed as a linear combination of others}. They are called linearly independent if they that cannot be done. Symbolically for a set of vectors $ v_1 $ $ v_2 $ ... $ v_n $ if we can find co-efficients $ \alpha_1 \alpha_2 ... \alpha_n $ such that \[ \alpha_1 * v_1  = \alpha_2 * v_2 + ... + \alpha_n * v_n \] then they are said to linearly dependent. If we cannot (as in there does not exist such set of co-efficients) then the set of vectors are called linearly independent.

%% Seems like a definition that is natural and will comes in handy.


%% \chapter{Span}
%% \textbf{The span of any set of vectors (F) is the set of all vectors (S) that can be formed using linear combination of each of the vectors in (F)}. Symbolically, the span of vectors $ v_1 $ $ v_2 $ ... $ v_n $ (F) is set of all vectors formed by $ \alpha_1 * v_1 + \alpha_2 * v_2 + ... + \alpha_n * v_n $ (S) where $ \alpha_1, \alpha_2 ... \alpha_n $ are any real numbers.

%% The term span means something is similar to the normal meaning of the word span and the verb spanning itself i.e. like sort of to cover something.

%% \chapter{Basis}
%% \textbf{Basis of a set of vectors (S) is the set of linearly INdependent vectors (F) that span (S) i.e. the span of (F) is a superset or exactly equal to (S)}. Notice that
%% \begin{enumerate}
%%   \item By using the word linearly independent we are sort of putting a restriction on number of vectors in the set
%% \end{enumerate}

%% \chapter{But why these definitions?}
%% The idea is that from `space' or `spatial' point of view, span and basis are sort like inverse things. Given basis we can find the span, given a span we can find the basis. This sort of comes from the natural drive to generate and break down, I guess.

\end{document}
