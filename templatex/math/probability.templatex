\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{probability}

${ add_statement(

'Experiment',

r'''
A set of procedures which when followed produces some outcome.
''',

r'''
Can be used to represent a lot of things food recipies, code, course registrations etc... The meaning of this is not just limited to experiments which involve physics or chemistry. They are just a class of experiments called physical and chemical experiments respectively. There can exist imaginary experiments like (surprise surprise!) adding two numbers in your head, calculating limits etc...
''',

r'''
\proofbydefinition
'''

) }



${ add_statement(

'Deterministic Experiment',

r'''
An @experiment@ where $ O_1, O_2, O_3, ... $ represent outcomes at when experiment is repeated 1, 2, 3 and so on number of times, is such that $ O_1, O_2, O_3, ... $ are pairwise identical no matter how many times it is repeated is called a deterministic experiment @identity@.
''',

r'''
Can be used to represent experiments like adding two numbers.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Stochastic Experiment',

r'''
An @experiment@ where $ O_1, O_2, O_3, ... $ represent outcomes at when experiment is repeated 1, 2, 3 and so on number of times, is such that $ \exists (i, j) \st O_i \not\equiv O_j $ is called a stochastic experiment @identity@.
''',

r'''
Note that outcome can remain same for some repetitions of the experiement.
Can be used to represent experiments like rolling a dice.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Outcome Space',

r'''
The outcome space of an @experiment@ is the @set@ of its all possible outcomes.
''',

r'''
A term to refer to all possible outcomes collectively.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Event',

r'''
An event $ E $ of an @experiment@ is a @subset@ of @outcome space@. An event is said to have occured $ \biimpl $ $ \exists $ outcome $ \in E $ that occured.
''',

r'''
The number of elements in an event can be 0, 1 or even total number of possible outcomes.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Event Space',

r'''
The event space of an @experiment@ is the @set@ of all possible @event@s of the experiement.
''',

r'''
A term to refer to all possible events collectively
''',

r'''
\proofbydefinition
'''

) }



${ add_statement(

'Probability',

r'''
The probability of an @event@ $ \lambda $ of an @experiment@ is the limiting value of the ratio $ \frac{Number-of-times-\lambda-occured}{Number-of-repetitions} $ as the experiment is the repeated infinitely many times. Denoted by $ P(\lambda) $.
''',

r'''
Probability gives a sense of power of an event when an experiment is conducted.
Say an experiement has two outcomes $ \alpha, \beta $ with probabilities 0.999999999999 and 0.000000000001 respectively.
But we can never say with certainity that when an experiment is conducted only $ \alpha $ shall occur.
Nor is the case when experiement is repeated say 100000000000 (any finite number) of times the corresponding ratios shall be 0.999999999999 and 0.000000000001, it can also happen that in all those repetitions $ \alpha $ never occurs.
This subtle nature of probability, that arises from the definition of limit itself, should be digested well.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Basic properties of probability',

r'''
Given S be the @outcome space@ and E, A, B be some @event@s of an @experiment@ X. The following properties hold @null set@.
\begin{enumerate}[nolistsep]
  \item $P(A) \ge 0 $
  \item $P(S) \equiv 1 $
  \item $ A \cap B \equiv \phi \impl P(A \cup B) \equiv P(A) + P(B)$
\end{enumerate}

''',

r'''
Some basic tools which can be played around with to create compound interesting tools
''',

r'''
\item
\paragraph{1} From the ratio in the definition of probability, wheneven an experiment is conducted the numerator (num. times event E occured until now) $ \ge 1 $ and denominator (num. of repetitions done) $ > 0 $

$ \impl $ the ratio is always $ \ge 0 $.

$ \impl $ the limiting value of the ratio P(A) is also $ \ge 0 $.
\item
\paragraph{2} When the event is identical to outcome space the ratio is identical to one.

$ \impl $ the limiting value of the ratio P(S) is also $ \equiv 1 $.
\item
\paragraph{3} When the intersection b/w two events A and B is a null set, the number of occcurances of event $ A \cup B $ $ \equiv $ number of occurances of A $ + $ number of occurances of B. Therefore the ratios at every repetition and thus limiting values of ratios are equal i.e. $ P(A \cup B) = P(A) + P(B) $.
'''

) }

${ add_statement(

'Mutually Exclusive Events',

r'''
Two @event@s A and B of an @experiment@ are mutually exclusive events $ \biimpl $ $ A \cap B \equiv \phi $ @null set@.
''',

r'''
For example in the experiement of throwing a dice the event of getting an even number and the event of getting an odd number are mutually exclusive events.
Note that they are not independent events, even both words are eerily similar.
Mutually exclusive mean that only one can occur at a given time.
Independent means both can occur at the same time.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Independent Events',

r'''
Two @event@s A and B of an @experiment@ are independent $ \biimpl $ $ P(A \cap B) \equiv P(A) * P(B) $ @probability@.
''',

r'''
For example in the experiement of throwing a dice the event of throwing a dice first time and the event of throwing a dice second time can be considered independent events.

One very important subtlety is that indeed there is no hard and fast rule that these events need to be independent.
The turbulance from the first time might effect other which might cause some dependence.
Even if that is not the case this dependence definition is not limited to the normal sense of the word.
We can always create an experiement (and thus its probability function) (however bizzare, but still an experiment) with say getting 2 on top in first repetition is event A and the same in second repetition is B such that $ P(A \cap B) \not\equiv P(A) * P(B) $.
Then by definition these events are not independent, even if no one talks about such kind of experiements.

Note that they are not mutually exclusive events, even though both words are eerily similar.
Mutually exclusive mean that only one can occur at a given time.
Independent means both can occur at the same time.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Collapsed Experiment',

r'''
A collapsed experiment of an @experiment@ E is a new experiment F where F's @outcome space@ is a @proper subset@ of the E's outcome space.
''',

r'''
Why not also consider a proper superset of the outcome space?
Something like an Expanded Experiement?!
Good question, the reason is because by definition outcome space is a universal set.
And any superset of universal set is itself, again by definition.
''',

r'''
\proofbydefinition
'''

) }



${ add_statement(

'Conditional Probability',

r'''
Let P(A) denotes probability of some @event@ A of original @experiment@ E.
The conditional probability of an event A given event B of E is the @probability@ of the event $ A \cap B $ in the @collapsed experiment@ F of E whose @outcome space@ $ \equiv $ B, denoted by P(A$\mid$B) and defined as $ \equiv \frac{P(A \cap B)}{P(B)} $.
''',

r'''
This definition has more profound significance than its appears.

$ A \cap B $ because even the collapsed experiement must be a subset of its outcome space (here B).
$ \frac{1}{P(B)} $ because it normalizes probabilities such that sum of probabilities of all outcomes = 1.

To be more precise the definition is actually
\[ P(A \mid B) \equiv \lim_{repetitions\to\infty} \frac{Number-of-times-A \cap B-occured}{Number-of-repetitions} \]
where the experiement repeated is the collapsed experiment.
There is no ultimate reason to follow the neat rule.
We can easily construct an experiment which does not obey the neat rule of
\[ P(A \mid B) \equiv \frac{P(A \cap B)}{P(B)} \]
For example consider an experiment with three outcomes each with probability 1/3.
If the experiment is collapsed to outcome space of only the second and third outcomes, there is no hard and fast rule that new probabilities become 1/2 as given by the neat rule.
They can even be 0.4 and 0.6 respectively and that is fine and can be captured by the limit definition.
The point is that this neat rule just does not represent such transformations.
They can be modeled by other rules.
The reason we choose this neat rule as definition is because of its simplicity and as we believe, simple is better.

By assuming that outcome space collapses to B, we are effectively changing probabilities of some events.
This is because even in collapsed experiement (which has a different collapsed outcome space) each probability $ \ge $ 0 and probability of the collapsed outcome space = 1.
More specifically speaking probabilities sort of are compressed in that now size of outcome space is decreased but still its probability = 1.
The changed probabilities are called conditional probabilities and are defined by whatever rule we follow (here it is that neat rule).
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Random Variable',

r'''
A random variable of an @experiment@ E is a @function@ $ \psi $ mapping from E's @event space@ S to $ [0, 1] $ where $ \forall \lambda \in S, \psi(\lambda) $ (the image of $ \lambda $ under the function) $ \equiv $ the @probability@ of the event $ \lambda $ and $ \forall \alpha, \beta \in S \st \beta \not\equiv \phi, \psi(\alpha\mid\beta) \equiv $ the conditional probability of event $ \alpha $ given event $ \beta $ for the experiment E.
''',

r'''
A function wrapper around the definitions of experiment, event space, probability.
Note that $ \psi(\alpha \mid \beta) $ is conditional probability of event $ \alpha $ given event $ \beta $ for the experiment E only, not the collapsed experiment of E.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Total Probability Theorem',

r'''
For a @random variable@ $\psi$ of an @experiment@ E, let A and B be some @event@s such that $\{A_1, A_2, ... A_n\}$ represents a @partition@ of a A such that none of the $ A_i $ is a @null set@ $ \impl $
\[
  \psi(B) = \sum_{i=1}^{n} \psi(B \mid A_i ) \psi ( A_i )
\]

''',

r'''
Using this theorem we can split the probability of event B into conditional probabilities of B given $ A_i $s and proababilities of $ A_i $s.
''',

r'''
'''

) }

${ add_statement(

'Bayes Theorem',

r'''
For a @random variable@ $\psi$ of an @experiment@ E, let A and B be some @event@s such that $\{A_1, A_2, ... A_n\}$ represents a @partition@ of a A such that none of the $ A_i $ is a @null set@ $ \impl $
\[
  \psi(A_i \mid B) = \frac{\psi(B \mid A_i ) \psi( A_i )}{\sum_{i=1}^{n} \psi(B \mid A_i ) \psi ( A_i )}
\]

''',

r'''
When you have a world that is constant but cannot be observed accurately at once and you can have evidence which determines something about the world, then you can update what you believe about the world by using this theorem. Videos by \href{https://youtu.be/HZGCoVF3YvM}{3blue1brown} and \href{https://youtu.be/R13BD8qKeTg}{veritasium} provide simple ideas to build the concept/definition of bayes theorem.
''',

r'''

'''

) }

\begin{enumerate}[nolistsep]
  \item \todo
  \item Cdf and pdf definitions
  \item Expected value definitions
  \item Joint probability definition
  \item Random vectors
  \item baysian networks
\end{enumerate}
\end{document}
