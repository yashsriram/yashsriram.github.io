\documentclass[../main.tex]{subfiles}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bvectwo}[2]{\begin{bmatrix} #1 \\ #2 \end{bmatrix}}
\newcommand{\bvecn}[1]{\begin{bmatrix} #1_1 \\ #1_2 \\ ... \\ #1_n \end{bmatrix}}
\newcommand{\bvecnsum}[2]{\begin{bmatrix} #1_1 + #2_1 \\ #1_2 + #2_2 \\ ... \\ #1_n + #2_n \end{bmatrix}}
\newcommand{\bvecnscale}[2]{\begin{bmatrix} #1 * #2_1 \\ #1 * #2_2 \\ ... \\ #1 * #2_n \end{bmatrix}}

\begin{document}

\chapter{vectors}

${ add_statement(

'Vector',

r'''
  A vector can be defined as a collection of @real numbers@ (also called elements) where the arrangement in which elements appear matters.
  For n elements a vector can be written as \[ V \equiv \bvecn{e} \]
''',

r'''
  The motivation behind vectors is to view a group of entities as a single entity.
  By viewing group of multiple entities as a single entity, a more abstract concept can be created, where instead of applying the same operation to each and every element, again and again, we apply the same operation to the whole group entity at once i.e. vector.
  A simple use case would be say when you need to update marks of all students to a 100 scale from a 10 scale.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Vector Dimension',

r'''
  The dimension of a @vector@ is just the count of elements in it.
  A vector with dimension N can be called N dimensional vector, written as ND vector.
''',

r'''
  A name for size of vector.
  Note that we are not considering collections with zero elements as vectors as it does not seem to be useful.
  The concept is illustrated in the table \ref{tab:dim}.
  \begin{table}[ht]
    \centering
    \begin{tabular}{ c  c  c }
      Vector name & Value & Dimension\\
      $ V_1 $ & (1) & 1\\
      $ V_2 $ & ($\sqrt{2}$) & 1\\
      $ V_3 $ & (-100, $\sqrt{3}$) & 2\\
      $ V_4 $ & (0, 0.1) & 2\\
      $ V_5 $ & (0, 0, 0) & 3\\
      $ V_6 $ & (0, 1, 2, 3) & 4\\
    \end{tabular}
  \caption{Dimensions of Vectors}
  \label{tab:dim}
  \end{table}
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Vector Addition',

r'''
  The addition of two ND @vector@s \[ A \equiv \bvecn{a} \] and \[ B \equiv \bvecn{b} \] is a new ND vector with each element as sum of corresponding elements in $A$ and $B$.
   @vector dimension@. Denoted by \[ A + B \equiv \bvecnsum{a}{b} \].
''',

r'''
  This is to re-enforce the notion of applying operation to the group entity rather than each and every element repeatedly.
  Otherwise there is no reason not to define the addition rule as something arbitrary like
  \[
    \bvectwo{x}{y} + \bvectwo{a}{b} \equiv \bvectwo{xa + yb^2}{\frac{b - y}{x + a}}
  .\]
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Vector Scaling',

r'''
  The scaling of an ND @vector@ \[ A \equiv \bvecn{a} \] @vector dimension@ with a real number @real numbers@ $\lambda$ produces a new ND vector with each element as product of corresponding elements in $A$ and $\lambda$. Denoted by \[ \lambda * A \equiv \bvecnscale{\lambda}{a} \].
''',

r'''
  Same story. Here $\lambda$ is called (surprise surprise) a scalar, a mysterious word that crept in the subject of vectors suddenly becomes not so mysterious after knowing its name is given to it by the work it does.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Linear Combination',

r'''
  Linearly combination two vectors ND @vector@s A and B @vector dimension@ means first scaling each vector with a real number @real numbers@ and adding resultant vectors @vector scaling@ @vector addition@.
  The produced vector is also N dimensional.
  Denoted by $ \alpha * A + \beta * B $, where $\alpha$ and $\beta$ are real numbers.
''',

r'''
  This is nothing new, just a word to combine one scaling and one adding operations.
  A higher level construct to play with.
  What is so linear about it?
  Well the word linear is given because there can exist other types of combinations like quadratic combination where before scaling the entities are squared, polynomial combination, exponential combination, logarithmic combination ...
  The most simplest of them all seems to be linear combination.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Linearly Independent Set',

r'''
A linearly independent set is a @set@ of @vector@s B of cardinality $>$ 1, such that $ \forall b_i \in B, b_i \not\equiv $ @linear combination@ of all other vectors in B.

A set of vectors in which at least one element $ \equiv $ a linear combination of all other elements is called a linearly dependent set of vectors.
''',

r'''
Vectors in a set with this property are sort of independent from others as in they cannot be built by other vectors using any simple operation available for vectors.
Yes! addition and scaling are all the available simple operations for vectors. Mind blown!
Well you might scream matrices. But matrices were created just for that purpose, to build a vector that is otherwise not constructable using addition and scaling.

Note that there cannot be a set of vectors in which one element $ b_d \equiv $ linear combination of all other elements and another element $ b_i \not\equiv $ linear combination of all other elements.
This is because given the first identity, we can rearrange terms to get second identity.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Span',

r'''
The span of a @set@ of @vector@s B is a set of vectors S in which each vector $ \equiv $ @linear combination@ of all vectors in B.
''',

r'''
A tool to give a sense of spread/cover of a set of vectors.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Basis',

r'''
A basis of a @set@ of @vector@s S is a @linearly independent set@ of vectors B, such that each vector in S $ \equiv $ @linear combination@ of all vectors in B.
''',

r'''
Sort of an inverse of span.
Note that for a given set of vectors there can be multiple basis sets.
For instance the set of all 2D vectors has the following sets as basis sets.
\[
  \{ \bvectwo{1}{0}, \bvectwo{0}{1} \}, \{ \bvectwo{2}{0}, \bvectwo{0}{2} \}, \{ \bvectwo{2}{0}, \bvectwo{1}{2} \}
\]
By using the word linearly independent we are sort of putting a restriction on number of vectors in the set.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Standard Vector',

r'''
A standard vector is a @vector@ in which value of one and only one element is one. If any elements are remaining their value is @zero@.
''',

r'''
An standard vector is a simple unit vector.
For example
\[
  \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}
  \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}
  \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
  \begin{bmatrix} 1 \end{bmatrix}
\]
are instances of standard vector.
It can be used as a building block for creating useful tools.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Standard basis',

r'''
The standard basis of a @set@ of N-dimensional @vector@s V is the set of all @standard vector@s of dimension N.
''',

r'''
Can be used to construct coordinate axes and thus coordinate system.
As each vector in standard basis belongs to one and only one coordinate axis, any point in the state vector space $ \equiv $ linear combination of all vectors in standard basis.
For example a set of 4D vectors have a standard basis as the following
\[
  \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}
  \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}
  \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}
  \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}
\]
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'L1 Norm',

r'''
The L1 norm of a @vector@ \[ V \equiv \bvecn{e} \] is defined as the sum of absolute values of elements of V. Denoted by \[ \norm{V}_1 \equiv \sum_{i = 1}^{n} abs(e_i) \].
''',

r'''
A simple and natural way to get a sense of size of vector.
This norm is also called Manhattan norm or Taxicab norm, as this represents the distance travelled by taxis to go from one point to another.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'L2 Norm',

r'''
The L2 norm of a @vector@ \[ V \equiv \bvecn{e} \] is defined as the square root of sum of squares of elements of V. Denoted by \[ \norm{V}_2 \equiv \sqrt{ \sum_{i = 1}^{n} e_i^2 } \].
''',

r'''
A simple and natural way to get a sense of size of vector.
This norm is also called Euclidean norm.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'L-infinity Norm',

r'''
The L-infinity norm of a @vector@ \[ V \equiv \bvecn{e} \] is defined as the maximum of absolute values of elements of V. Denoted by \[ \norm{V}_\infty \equiv \max_{i = 1}^{n}(abs(e_i)) \].
''',

r'''
Gives a sense of size of vector.
For example, if the vector represents the cost of constructing a buildings, by minimizing L-infinity norm we are reducing the cost of the most expensive building.
''',

r'''
\proofbydefinition
'''

) }
${ add_statement(

'Unit Vector',

r'''
A @vector@ V whose @L2 norm@ $ \norm{V}_2 \equiv 1 $ is called a unit vector.
''',

r'''
A unit vector has magnitude of one in the L2 norm sense.
Therefore it can be used as a template vector, which can be scaled to create new vectors of arbitrary size.
Ofcourse vector of magnitude other than one can also be used for this purpose.
But the magnitude being one makes it simple to get an arbitrary magnitude M vector, just by scaling the unit vector by M.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Dot Product',

r'''
The dot product of N-dimensional two @vector@s
\[
  A \equiv \bvecn{a}
  B \equiv \bvecn{b}
\]
is a real number @real numbers@ $ \eta \equiv $ sum of element-wise products of A and B.
Denoted by
\[
  A . B \equiv \sum_{i = 1}^{n} (a_i * b_i)
\]
''',

r'''
A tool to give a sense of similarity of vectors.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Angle between Vectors',

r'''
The angle between two N-dimensional @vector@s A and B is a real number $ \theta \equiv $ arc cosine of @dot product@ of vectors divided by @L2 norm@ of both A and B. Denoted by
\[
  \angle(A,B) \equiv arccos(\frac{A.B}{\norm{A}_2 * \norm{B}_2})
\]
''',

r'''
A tool give a sense of orientation difference b/w the vectors.
''',

r'''
\proofbydefinition
'''

) }

${ add_statement(

'Perpendicular Vectors',

r'''
Two N-dimensional @vector@s A and B are said to be perpendicular vectors $ \biimpl $ the cosine of @angle between vectors@ $ \equiv 0 $.
''',

r'''
Gives a sense of independence b/w two vectors.

In vectors of dimensions 2 and 3, we visualize this by drawing what we usually draw for coordinate systems.
This visualization is just a choice.
We choose that because the independence is visualized in the sense of projections.
In 1D there are only one axis and in 4D and more there is no easy, simple and useful way to visualize perpendicular axes yet.
So there is no need to visualize them in 4D or more dimensional spaces.
''',

r'''
\proofbydefinition
'''

) }



\end{document}

